下面给你把 **两种接口**（`/v1/chat/completions` 和 `/v1/completions`）各做成 **两种调用形式**：**Python 脚本** + **.sh 脚本**，并给出**访问方法**（如何运行、如何改地址/模型名）。

我默认你的 vLLM OpenAI server 已经启动在：`http://127.0.0.1:8000`
（如果不是，改下面脚本里的 `BASE_URL` / `HOST` / `PORT` 即可）

---

## 0) 通用：先确认服务可访问

### 用 curl 看模型列表（可选）

```bash
curl http://127.0.0.1:8000/v1/models
```

如果能返回 JSON（models 列表），说明服务没问题。

---

# A. 调用 /v1/chat/completions（Chat 接口）

## A1) Python 脚本：chat_completions.py

```python
#!/usr/bin/env python3
import os
import json
import requests

BASE_URL = os.getenv("BASE_URL", "http://127.0.0.1:8000")
MODEL = os.getenv("MODEL", "Qwen2-7B-Instruct")

url = f"{BASE_URL}/v1/chat/completions"
payload = {
    "model": MODEL,
    "messages": [
        {"role": "system", "content": "你是一个有帮助的助手。"},
        {"role": "user", "content": "用一句话解释 vLLM 的优势。"}
    ],
    "temperature": 0.7,
    "max_tokens": 200,
    "stream": False
}

resp = requests.post(url, json=payload, timeout=120)
resp.raise_for_status()
data = resp.json()

# OpenAI compatible: choices[0].message.content
print(data["choices"][0]["message"]["content"])
```

### 运行方法

```bash
pip install requests
python3 chat_completions.py
```

### 自定义访问地址/模型名（可选）

```bash
BASE_URL="http://<你的IP>:8000" MODEL="你的模型名" python3 chat_completions.py
```

---

## A2) .sh 脚本：chat_completions.sh

```bash
#!/usr/bin/env bash
set -euo pipefail

HOST="${HOST:-127.0.0.1}"
PORT="${PORT:-8000}"
MODEL="${MODEL:-Qwen2-7B-Instruct}"

curl -s "http://${HOST}:${PORT}/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d "{
    \"model\": \"${MODEL}\",
    \"messages\": [
      {\"role\": \"system\", \"content\": \"你是一个有帮助的助手。\"},
      {\"role\": \"user\", \"content\": \"用一句话解释 vLLM 的优势。\"}
    ],
    \"temperature\": 0.7,
    \"max_tokens\": 200
  }" | python3 -m json.tool
```

### 运行方法

```bash
chmod +x chat_completions.sh
./chat_completions.sh
```

### 改 HOST/PORT/MODEL（可选）

```bash
HOST="192.168.1.10" PORT="8000" MODEL="你的模型名" ./chat_completions.sh
```

---

# B. 调用 /v1/completions（Prompt 补全接口）

## B1) Python 脚本：completions.py

```python
#!/usr/bin/env python3
import os
import requests

BASE_URL = os.getenv("BASE_URL", "http://127.0.0.1:8000")
MODEL = os.getenv("MODEL", "Qwen2-7B-Instruct")

url = f"{BASE_URL}/v1/completions"
payload = {
    "model": MODEL,
    "prompt": "vLLM 的核心优势是：",
    "temperature": 0.7,
    "max_tokens": 200,
    "stream": False
}

resp = requests.post(url, json=payload, timeout=120)
resp.raise_for_status()
data = resp.json()

# OpenAI compatible: choices[0].text
print(data["choices"][0]["text"])
```

### 运行方法

```bash
pip install requests
python3 completions.py
```

### 自定义访问地址/模型名（可选）

```bash
BASE_URL="http://<你的IP>:8000" MODEL="你的模型名" python3 completions.py
```

---

## B2) .sh 脚本：completions.sh

```bash
#!/usr/bin/env bash
set -euo pipefail

HOST="${HOST:-127.0.0.1}"
PORT="${PORT:-8000}"
MODEL="${MODEL:-Qwen2-7B-Instruct}"

curl -s "http://${HOST}:${PORT}/v1/completions" \
  -H "Content-Type: application/json" \
  -d "{
    \"model\": \"${MODEL}\",
    \"prompt\": \"vLLM 的核心优势是：\",
    \"temperature\": 0.7,
    \"max_tokens\": 200
  }" | python3 -m json.tool
```

### 运行方法

```bash
chmod +x completions.sh
./completions.sh
```

### 改 HOST/PORT/MODEL（可选）

```bash
HOST="192.168.1.10" PORT="8000" MODEL="你的模型名" ./completions.sh
```

---

## 额外：如果你要“流式输出”（可选加分）

> 你刚刚的需求没强制要 streaming，但很多课件会需要演示。
> 如果你要，我可以把 `chat_stream.py` 和 `chat_stream.sh` 也补上（SSE 逐行解析）。

你现在的 vLLM 服务启动命令里，模型名字实际是什么？（`/v1/models` 返回里的 id）如果你不确定，把返回贴出来我就把脚本里的 `MODEL` 替你换成正确的。
