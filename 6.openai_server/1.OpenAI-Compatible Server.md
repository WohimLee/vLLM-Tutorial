
## 第 6 章：OpenAI-Compatible Server（核心实战）

> 本章目标：
> **把一个本地大模型，用 vLLM 启动成一个“像 OpenAI 一样”的推理服务**，并理解它在工程上的价值。



### 6.1 为什么要 OpenAI 接口

#### 6.1.1 什么是 OpenAI-Compatible Server

vLLM 提供了一个 **完全兼容 OpenAI API 协议** 的 HTTP Server，包括：

* `/v1/completions`
* `/v1/chat/completions`
* 流式输出（SSE）

这意味着：

> **任何原本调用 OpenAI API 的应用，几乎不用改代码，就可以切到 vLLM 本地模型**



#### 6.1.2 生态兼容（最重要的原因）

OpenAI API 已经成为事实上的 **LLM 应用标准接口**：

* 前端：

  * Chat UI
  * Web / Mobile
* 后端：

  * LangChain
  * LlamaIndex
  * AutoGPT / Agent 框架
* 中间层：

  * API Gateway
  * 权限 / 限流 / 计费

📌 **一旦兼容 OpenAI API，你就直接接入了整个生态**



#### 6.1.3 前后端解耦（工程角度）

OpenAI 接口天然实现了：

* **模型实现与业务解耦**
* **推理后端可随时替换**

例如：

```text
前端 / 业务代码
    ↓
OpenAI API 协议
    ↓
vLLM / OpenAI / 其他推理后端
```

这样做的好处：

* 不关心模型是 GPT-4 还是 LLaMA
* 不关心推理是单卡还是多卡
* 不关心是云 API 还是本地 GPU



#### 6.1.4 为什么 vLLM 特别适合做 Server

vLLM 的核心能力 **天然为服务而生**：

* Continuous Batching → 高并发
* PagedAttention → 显存友好
* 内置 Scheduler → 请求调度公平
* 支持 Streaming → Chat 体验好

👉 **vLLM ≠ 写脚本推理，而是推理系统**



### 6.2 启动 vLLM Server（核心实战）



#### 6.2.1 最简单的启动方式

```bash
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-2-7b-chat-hf
```

启动后：

* 默认监听：`http://localhost:8000`
* 自动提供 `/v1/*` 接口
* 模型加载完成后即可请求



#### 6.2.2 常用启动参数详解（必讲）

##### （1）模型相关

```bash
--model <model_path_or_name>
```

* HuggingFace 模型名
* 本地模型路径
* 支持 LoRA / AWQ / GPTQ（取决于版本）



##### （2）上下文长度

```bash
--max-model-len 4096
```

含义：

* 单个请求允许的 **最大 token 总数**
* 直接影响：

  * KV Cache 显存
  * 是否 OOM

📌 **这是服务稳定性的关键参数**



##### （3）显存使用率（非常重要）

```bash
--gpu-memory-utilization 0.9
```

含义：

* vLLM 最多使用 GPU 显存的百分比
* 剩余显存留给：

  * CUDA runtime
  * 通信 buffer
  * 系统安全余量

经验值：

* 单卡部署：0.85～0.9
* 多卡 TP：0.8～0.85


##### （4）并发控制

```bash
--max-num-seqs 128
```

含义：

* 同时 **decode 的最大请求数**
* 相当于“最大并发”

⚠️ 太大：

* 延迟升高
* 显存压力增大

⚠️ 太小：

* 吞吐受限



##### （5）Tensor Parallel（多卡）

```bash
--tensor-parallel-size 2
```

* 将模型参数切分到多张 GPU
* **不是数据并行**
* 所有 GPU 共同服务一个模型实例



#### 6.2.3 一个「推荐的生产启动示例」

```bash
python -m vllm.entrypoints.openai.api_server \
  --model /models/Qwen2-7B-Instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.85 \
  --max-num-seqs 64 \
  --tensor-parallel-size 1 \
  --port 8000
```



#### 6.2.4 vLLM Server 的模型加载流程（原理）

1. 解析启动参数
2. 加载 tokenizer
3. 加载模型权重（分 GPU / TP）
4. 初始化 KV Cache Block Pool
5. 启动 Scheduler
6. 开启 HTTP 服务

📌 **模型只加载一次，所有请求复用同一个 Engine**



### 6.3 API 使用（实战）


#### 6.3.1 `/v1/chat/completions`（最常用）

>请求示例（curl）

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2-7B-Instruct",
    "messages": [
      {"role": "system", "content": "你是一个有帮助的助手"},
      {"role": "user", "content": "什么是 vLLM？"}
    ],
    "temperature": 0.7,
    "max_tokens": 512
  }'
```



##### 参数说明（重点）

| 参数          | 作用               |
| ----------- | ---------------- |
| messages    | Chat 上下文         |
| temperature | 随机性              |
| max_tokens  | 本次生成 token 上限    |
| top_p       | nucleus sampling |
| stream      | 是否流式输出           |



#### 6.3.2 `/v1/completions`（传统补全）

适合：

* 代码补全
* 模板填充
* 非 Chat 场景

```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2-7B-Instruct",
    "prompt": "vLLM 的核心优势是",
    "max_tokens": 200
  }'
```

#### 6.3.3 流式输出（Streaming）

>请求方式

```json
{
  "stream": true
}
```


##### 返回形式（SSE）

```text
data: {"choices":[{"delta":{"content":"vLLM"}}]}

data: {"choices":[{"delta":{"content":" 是一个"}}]}

data: {"choices":[{"delta":{"content":" 高性能推理框架"}}]}

data: [DONE]
```



##### 为什么 Streaming 很重要

* 用户体验更好（秒出字）
* 长回答不会阻塞连接
* 前端可以实时渲染

📌 **Chat 产品几乎必开 streaming**



### 6.4 常见问题与踩坑（课件加分项）

#### Q1：为什么 server 启动很慢？

* 模型权重加载
* CUDA kernel warmup
* 正常现象（冷启动）



#### Q2：并发高了反而变慢？

* max-num-seqs 过大
* KV Cache 竞争
* GPU 已满负载



#### Q3：能不能多个模型？

* vLLM 原生：**一个进程一个模型**
* 多模型：多进程 + 负载均衡



## 本章小结（Slide 总结页）

✅ vLLM 提供 **OpenAI-compatible 推理服务**
✅ 一行命令即可部署本地大模型 API
✅ 支持 Chat / Completion / Streaming
✅ 是生产级 LLM 服务的核心形态
