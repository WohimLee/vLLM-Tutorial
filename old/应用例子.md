
## 组合范式与样例拓扑（推理场景）

### 1 
##### 单机 8 卡，13B–34B 级模型

* 目标：高吞吐、低时延。
* vLLM：`TP=4` 或 `TP=8`；开启连续批与 chunked prefill；DP=1。
* SGLang：`TP=4`；提升批处理与缓存命中。
* LMDeploy：TurboMind `TP=4`；构建与运行保持一致。

##### 多机 64 卡，70B–405B 级模型

* 目标：跨机扩展吞吐。
* vLLM：Ray 多 worker 形成 **DP=K**，每 worker 内 `TP=8`（必要时 `PP=2`）；统一 API 网关。
* SGLang：多副本（DP）+ 每副本 `TP=8`；网关按会话粘性路由。
* LMDeploy：按集群划分 **DP 组**；每组内部 `TP=8/PP=2`；统一接入层。

##### MoE 推理（专家较少，超大参数）

* 原则：**EP 与 TP 组对齐**，优先单机内路由；若必须跨机，使用 **group‑wise all‑to‑all**；
* vLLM / LMDeploy：按各自 MoE 支持配置专家组；SGLang 视版本能力启用。




### 配置食谱：从机器与模型出发

实战拓扑案例

##### 案例 1：8 卡部署 LLaMA‑13B 推理（低延迟）

* 框架：vLLM
* 配置：`TP=4, PP=2, DP=1`
* 结果：4 卡并行处理层内大矩阵，2 段流水线切分，吞吐与显存平衡。

##### 案例 2：32 卡部署 Mistral‑7B 大规模服务（高吞吐）

* 框架：SGLang
* 配置：`TP=2, DP=16`
* 结果：16 个副本并行服务，每副本内部 2 卡张量并行，高效批处理，服务百路并发。

##### 案例 3：16 卡部署 InternLM‑20B 长上下文（8192 tokens）

* 框架：LMDeploy
* 配置：`TP=4, PP=2, DP=2`
* 结果：流水线降低显存压力，张量并行分解大矩阵，DP 提升吞吐，支持长上下文。

