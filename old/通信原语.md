## 通信原语
这些原语主要出现在 `分布式训练 / 大模型并行化` 场景下，用于多 `GPU / 多节点`之间传递 `梯度（grad）` 或 `激活值（activation）`

### 1 All Reduce（grad）

作用：对所有 GPU 的梯度进行规约（Reduce，如求和/平均），然后把结果广播回每个 GPU。

典型场景：数据并行训练中，多个 GPU 各自计算梯度后，需要同步成全局梯度。

开销：通信量与模型参数量成正比。

### 2 P2P（stage 间激活／梯度）

作用：点对点通信（Point-to-Point）。常用于流水线并行（Pipeline Parallelism）。

典型场景：
- 前向时，下游 stage 需要上游 stage 的激活值
- 反向时，上游 stage 需要下游 stage 的梯度

特点：通信只发生在相邻的 stage，数据量通常为激活大小，而非全参数大小。

### 3 AllReduce／AllGather

AllReduce：见上，主要同步梯度。

AllGather：将每个 GPU 上的张量拼接（gather）成一个大张量，并复制到所有 GPU 上。

典型场景：
- Tensor 并行（TP）里需要重建完整的输入或输出张量时
- Transformer 的 attention 机制里经常需要 AllGather

### 4 AllGather／ReduceScatter

AllGather：拼接张量，广播到所有设备。

ReduceScatter：先做规约（如加和），再把结果分片（scatter）给不同 GPU。

特点：这两个经常配合使用，可以实现功能类似 AllReduce，但更灵活。

应用：如 ZeRO 优化里梯度和参数的切分／重建。

### 5 All-to-All ＋ grad AllReduce

All-to-All：每个 GPU 把自己的数据分片发送给所有其他 GPU，每个 GPU 同时接收其他 GPU 的不同分片。

应用：
- MoE（Mixture of Experts）训练时，token routing 经常需要 All-to-All
- 再结合 梯度 AllReduce，用于同步 expert 权重的梯度

### 6 P2P ＋ AllReduce

P2P：stage 间传激活／梯度（流水线并行）

AllReduce：每个 stage 内的数据并行需要做梯度同步

典型场景：混合并行（Pipeline + Data Parallel）

### 7 All-to-All

作用：所有 GPU 之间的数据交换，每个 GPU 既是发送方又是接收方。

应用：

- MoE 模型（不同 token 需要路由到不同的 expert）
- 通信量大，但在专家稀疏化场景下比全量 dense 模型更划算

### 总结

- AllReduce → 最常见，用于 数据并行梯度同步
- P2P → 流水线并行里传 激活/梯度
- AllGather / ReduceScatter → 常用于 张量并行，重建或切分张量
- All-to-All → 常用于 MoE 并行，做 token/expert 的交换
- 组合方式（P2P+AllReduce，All-to-All+grad AllReduce 等） → 对应混合并行策略