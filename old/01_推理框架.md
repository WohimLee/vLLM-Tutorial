## 推理框架
>查看自己的硬件环境
```sh
python - <<'PY'
import torch, platform
print("Torch:", torch.__version__)
print("CUDA (compiled):", torch.version.cuda)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("Device:", torch.cuda.get_device_name(0))
    print("Compute capability:", torch.cuda.get_device_capability(0))
PY

nvidia-smi | sed -n '1,15p'   # 看驱动版本、GPU 型号
python -c "import sgl_kernel, flashinfer, sys; print('sgl_kernel OK'); print('flashinfer OK')"
```
### 1 vLLM

>特点

- 专注于高效推理和服务，采用 `PagedAttention` 技术，能在有限显存下高效支持大批量并发请求
- 对 多用户共享 GPU 资源 场景友好
- 提供 OpenAI API 兼容接口，方便集成到现有应用
- 常用于 `在线推理服务`，特别是大规模部署场景

>优势
- 吞吐量高、延迟低、显存管理优化

>不足
- 目前主要聚焦在 `推理（Inference）`，不涉及 `训练` 或 `参数微调`

### 2 SGLang

>特点

- 面向大模型 `对话、函数调用、结构化输出` 的推理框架
- 支持 编程式提示（programmable prompting），更适合 `Agent 类` 应用
- 优化了 KV Cache 管理，支持 推理流式输出

>优势
- 在 `复杂交互（如工具调用、函数调用、代码生成）` 的应用里体验更好

>不足
- 生态相对年轻，部署规模化案例比 vLLM 少

### 3 LMDeploy

开发方：由 OpenMMLab 团队（上海 AI Lab） 开发。

>特点

- 提供 `推理、量化、部署` 一体化工具链
- 支持 `多种硬件后端（NVIDIA GPU、Ascend、CUDA、Triton、TensorRT）`
- 内置 `TurboMind 引擎`，高效管理 KV Cache，优化批量推理
- 提供从模型下载、量化、部署到 API 服务的全套流程
- 支持 `INT4/INT8/FP16/FP32` 多精度推理，兼顾性能和效果
- 提供和 vLLM 类似的 `OpenAI API 接口`，方便对接应用

>优势

- 一体化（量化 + 推理 + 部署），非常适合企业快速上线
- 对国产硬件（Ascend、昆仑芯等）兼容性更好
- `KV Cache 管理` 和 `TensorRT` 深度集成，性能表现接近 `vLLM / TensorRT-LLM`

>不足
- 社区生态和应用案例相对 vLLM、TGI 少一些

### 4 XInference

>特点

- 由 Xorbits 社区推出，支持 多后端（包括 vLLM、Transformers、TGI、llama.cpp 等）
- 注重 灵活性，支持多模型、多任务混合部署
- 提供 统一 API，用户可以透明切换推理引擎

>优势
- 兼容性强，适合想在一个系统里管理多个推理引擎/模型的用户

>不足
- 在极致性能上不一定比 vLLM / TGI 更优

### 5 Ollama

>特点
- 偏向 桌面/本地部署 的推理框架，尤其是 macOS、Linux、Windows 用户
- 提供简单的 Ollama run 方式运行 LLaMA、Mistral 等模型
- 支持 模型打包（Modelfile），方便分发和部署
- 常见于个人开发者、轻量级本地 Agent

>优势
- 易用性高，快速启动，无需复杂环境
>不足
- 主要适合本地单机，不太适合大规模企业级分布式部署

### 6 其他常见推理框架

#### TGI (Text Generation Inference)

- Hugging Face 出品，企业常用的生产级推理框架
- 支持 多 GPU 并行、量化、模型并行
- 生态和社区活跃度高

#### llama.cpp

- 用 C++ 编写，支持 CPU/GPU，本地运行 LLaMA 系列模型非常轻量
- 适合移动端、嵌入式设备

#### TensorRT-LLM

- NVIDIA 出品，专注 GPU 加速，极致性能优化
- 对于大规模生产部署（尤其是 A100/H100 GPU 集群）非常合适

### 7 总结对比

| 框架               | 定位/场景         | 性能优化                    | 易用性 | 部署规模   | 特色              |
| ---------------- | ------------- | ----------------------- | --- | ------ | --------------- |
| **vLLM**         | 在线推理服务、大规模部署  | 高 (PagedAttention)      | 中   | 大规模分布式 | 高吞吐、低延迟         |
| **SGLang**       | Agent 应用、函数调用 | 中 (KV Cache 优化)         | 中   | 中      | 结构化输出/函数调用      |
| **XInference**   | 多后端统一接口       | 中                       | 中   | 中      | 兼容性、灵活性         |
| **Ollama**       | 本地部署、个人开发     | 中                       | 高   | 小      | 简单、轻量           |
| **TGI**          | 企业级生产部署       | 高                       | 中   | 大      | Hugging Face 生态 |
| **llama.cpp**    | 本地轻量化场景       | 中                       | 高   | 小      | 跨平台、CPU/GPU     |
| **TensorRT-LLM** | 极致 GPU 优化     | 极高                      | 低   | 大规模    | NVIDIA 官方优化     |
| **LMDeploy**     | 推理+量化+部署一体化   | 高 (TurboMind, TensorRT) | 中   | 大规模    | 一体化工具链，国产硬件支持   |
