## vLLM å…¥é—¨ä¸å®è·µ
- å®˜æ–¹ User Guide: https://docs.vllm.ai/en/stable/usage/index.html

>ğŸ“¦ å®‰è£… vLLMï¼ˆéœ€è¦ CUDA ç¯å¢ƒï¼‰
```
pip install vllm
```


---
### ğŸ§© ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€è®¤çŸ¥ â€” æ­å»ºä¸è¿è¡Œ

>ç›®æ ‡ï¼šç†è§£ vLLM æ˜¯ä»€ä¹ˆã€èƒ½å¹²å˜›ã€æ€ä¹ˆè·‘èµ·æ¥ã€‚

##### 1 vLLM åŸºæœ¬æ¦‚å¿µ
- vLLM çš„å®šä½ï¼šé«˜æ•ˆçš„ LLM æ¨ç†æ¡†æ¶ï¼ˆé‡ç‚¹æ˜¯ PagedAttention + é«˜åå KV cache ç®¡ç†ï¼‰
- ä¸ HuggingFace Transformersã€DeepSpeedã€Triton çš„å…³ç³»
- æ ¸å¿ƒæ¨¡å—ï¼šEngine, Worker, Scheduler, PagedAttention

##### 2 å®‰è£…ä¸è¿è¡Œ
- ç¯å¢ƒè¦æ±‚ï¼šCUDAã€PyTorch ç‰ˆæœ¬åŒ¹é…ã€é©±åŠ¨ç‰ˆæœ¬
- å®‰è£…æ–¹å¼ï¼š`pip install vllm` / ä»æºç æ„å»º
- åŸºæœ¬å‘½ä»¤ï¼š
    ```sh
    vllm serve model_name
    vllm generate --prompt "Hello world"
    ```
- è®¤è¯†å¸¸ç”¨å‚æ•°ï¼š--tensor-parallel-size, --gpu-memory-utilization, --max-model-len, --attention-backend

#####  3 åŸºæœ¬è°ƒè¯•ä¸éƒ¨ç½²

- æŸ¥çœ‹ `GPU` å ç”¨ä¸åå
- ç†è§£æ—¥å¿—ä¿¡æ¯ï¼ˆå¦‚ä½ ä¸Šæ¬¡è´´çš„é‚£ç§å¯åŠ¨æ—¥å¿—ï¼‰
- `Web API` è°ƒç”¨ï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰

---
### âš™ï¸ ç¬¬äºŒé˜¶æ®µï¼šæ ¸å¿ƒæœºåˆ¶ä¸æ¶æ„

>ç›®æ ‡ï¼šç†è§£ `vLLM` ä¸ºä»€ä¹ˆå¿«ã€æ€ä¹ˆå®ç°é«˜ååã€ä¸æ™®é€šæ¨ç†æ¡†æ¶çš„ä¸åŒã€‚

##### 1 å†…æ ¸æœºåˆ¶ï¼šPagedAttention

- `KV Cache` çš„ä½œç”¨
- ä¼ ç»Ÿå®ç°çš„ç“¶é¢ˆï¼š`O(NÂ²) memory` / å¤åˆ¶
- `vLLM` çš„åˆ†é¡µæ€æƒ³ï¼š`Page Manager` / `Block Table` / `Virtualized KV Cache`
- `Memory Pool` ä¸ `Page Table` çš„æ˜ å°„æœºåˆ¶
- åŠ¨æ€æ‰¹å¤„ç†ï¼ˆContinuous batchingï¼‰

##### 2 Scheduler ä¸ä»»åŠ¡å¹¶å‘

- è¯·æ±‚é˜Ÿåˆ—ä¸ `Batch` åˆå¹¶ç­–ç•¥
- `Prefill` / `Decode` åˆ†é˜¶æ®µè°ƒåº¦
- è¯·æ±‚æŠ¢å ä¸åŠ¨æ€æ‰¹æ¬¡é‡ç»„
- `Streaming` è¾“å‡ºæœºåˆ¶

##### 3 Attention Backendï¼ˆåç«¯å®ç°ï¼‰

- å„åç«¯åŸç†ä¸æ¯”è¾ƒ

    - FlashAttentionï¼ˆé«˜æ€§èƒ½ï¼‰
    - Tritonï¼ˆé€šç”¨ï¼‰
    - FlexAttentionï¼ˆå¯æ‰©å±•ï¼‰
    - xFormers / Torchï¼ˆå…¼å®¹å…œåº•ï¼‰
- åç«¯é€‰æ‹©ç­–ç•¥ã€ç¯å¢ƒå˜é‡é…ç½®

---

### ğŸš€ ç¬¬ä¸‰é˜¶æ®µï¼šæ€§èƒ½ä¼˜åŒ–ä¸éƒ¨ç½²æŠ€å·§

>ç›®æ ‡ï¼šå­¦ä¼šè®© vLLM è·‘å¾—æ›´å¿«ã€æ›´çœæ˜¾å­˜ã€æ›´ç¨³å®šã€‚

##### 1 æ€§èƒ½è°ƒä¼˜

- `--gpu-memory-utilization` å½±å“
- æ‰¹é‡å¤§å°ä¸ä¸Šä¸‹æ–‡é•¿åº¦çš„å¹³è¡¡
- å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelï¼‰
- å¤š `GPU` å¯åŠ¨æ¨¡å¼
- æ··åˆç²¾åº¦ï¼ˆ`FP16 / BF16 / INT8`ï¼‰

##### 2 æ¨¡å‹å…¼å®¹æ€§ä¸é‡åŒ–

- æ”¯æŒçš„æ¨¡å‹æ¶æ„ï¼š`Llama, Mistral, Qwen, Phi, Gemma` ç­‰
- é‡åŒ–æ”¯æŒï¼š`AWQ, GPTQ, FP8` ç­‰ï¼ˆ`vllm.quantization`ï¼‰
- `vLLM + LoRA`ï¼ˆAdapter æ”¯æŒï¼‰

##### 3 æœåŠ¡åŒ–ä¸é›†ç¾¤

- `RESTful / OpenAI API` éƒ¨ç½²
- `vLLM + FastAPI / Gradio / LangChain`
- åˆ†å¸ƒå¼éƒ¨ç½²ï¼ˆ`Ray / Kubernetes / Triton Server`ï¼‰
- é«˜å¹¶å‘ / å¤šç§Ÿæˆ·ç­–ç•¥

---
### ğŸ§  ç¬¬å››é˜¶æ®µï¼šæ·±å…¥ç†è§£ä¸æ‰©å±•

ç›®æ ‡ï¼šèƒ½å¤Ÿä¿®æ”¹ã€æ‰©å±•ã€ç”šè‡³è´¡çŒ® vLLMã€‚

##### 1 ä»£ç æ¶æ„é˜…è¯»

- `vllm/core/` æ ¸å¿ƒä»£ç ç»“æ„
- `vllm/engine/`ï¼šEngine + Scheduler
- `vllm/attention/`ï¼šPagedAttention backend
- `vllm/model_executor/`ï¼šæ¨¡å‹åŒ…è£…ä¸æ‰§è¡Œ
- `vllm/worker/`ï¼šå¤šè¿›ç¨‹ä¸ GPU è°ƒåº¦

##### 2 æ’ä»¶ä¸è‡ªå®šä¹‰

- è‡ªå®šä¹‰ `logits processor / sampler`
- è‡ªå®šä¹‰æ¨¡å‹åŠ è½½ï¼ˆé `HuggingFace` æ¨¡å‹ï¼‰
- æ·»åŠ æ–°çš„ `Attention backend`ï¼ˆå¦‚ `FlexAttention` å®éªŒï¼‰
- ä¿®æ”¹ `KV` ç®¡ç†ç­–ç•¥ï¼ˆ`block` å¤§å°ã€åˆ†é¡µç­–ç•¥ï¼‰

##### 3 æ·±å…¥é˜…è¯»ææ–™

- ğŸ“˜ å®˜æ–¹æ–‡æ¡£ï¼šhttps://docs.vllm.ai
- ğŸ“„ è®ºæ–‡ï¼š
    - â€œvLLM: Easy, Fast, and Cheap LLM Serving with PagedAttentionâ€ (arXiv 2023)
    - â€œFlashAttentionâ€ / â€œFlexAttentionâ€ papers

- ğŸ“‚ æºç å­¦ä¹ è·¯çº¿ï¼šä» engine.py â†’ scheduler.py â†’ attention.py
- ğŸ§© GitHub é¡¹ç›®ï¼šhttps://github.com/vllm-project/vllm
- å¸¸çœ‹ Issuesã€PRã€Discussions äº†è§£æ¼”è¿›æ–¹å‘